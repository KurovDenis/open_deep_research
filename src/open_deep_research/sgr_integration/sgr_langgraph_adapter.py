"""SGR LangGraph Adapter - –º–æ—Å—Ç –º–µ–∂–¥—É SGR streaming –∏ LangGraph workflow"""

import asyncio
import json
from typing import Dict, Any, Optional, Literal, Union
from langchain_core.runnables import RunnableConfig
from langchain_core.messages import HumanMessage, AIMessage
from langgraph.types import Command

from open_deep_research.configuration import Configuration
from open_deep_research.state import AgentState
from open_deep_research.utils import get_today_str

# –ò–º–ø–æ—Ä—Ç—ã SGR –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ (—Ç–µ–ø–µ—Ä—å –¥–æ—Å—Ç—É–ø–Ω—ã)
try:
    from ..sgr_streaming.enhanced_streaming import enhanced_streaming_display, EnhancedSchemaParser
    from ..sgr_streaming.sgr_visualizer import SGRLiveMonitor
    from ..sgr_streaming.sgr_streaming import SGRAgent, NextStep
    from rich.console import Console
    SGR_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è  SGR streaming components not found: {e}")
    SGR_AVAILABLE = False
    Console = None


class SGRStreamingNode:
    """LangGraph —É–∑–µ–ª —Å SGR streaming –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π"""
    
    def __init__(self, config_dict: dict = None):
        self.config = config_dict or {}
        self.streaming_enabled = self.config.get("streaming_enabled", True)
        self.current_step = 0
        self.max_steps = self.config.get("max_reasoning_steps", 4)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º SGR –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
        if SGR_AVAILABLE:
            self.console = Console()
            self.monitor = SGRLiveMonitor(self.console)
            self.parser = None
        else:
            self.console = None
            self.monitor = None
            self.parser = None
    
    async def __call__(self, state: AgentState, config: RunnableConfig) -> Command:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç SGR reasoning step —Å streaming –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º"""
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            configurable = Configuration.from_runnable_config(config)
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            messages = state.get("messages", [])
            if not messages:
                return Command(goto="clarify_with_user")
            
            user_message = messages[-1].content if hasattr(messages[-1], 'content') else str(messages[-1])
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º SGR reasoning —Å streaming
            if self.streaming_enabled and SGR_AVAILABLE:
                return await self._execute_with_streaming(user_message, state, configurable)
            else:
                return await self._execute_simple(user_message, state, configurable)
        
        except Exception as e:
            print(f"Error in SGR node: {e}")
            # Fallback –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –ø–æ–≤–µ–¥–µ–Ω–∏—é
            return Command(goto="research_supervisor")
    
    async def _execute_with_streaming(self, user_message: str, state: AgentState, config: Configuration) -> Command:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å SGR streaming –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º"""
        
        decision = await self._analyze_research_need(user_message, config)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞
        if "clarification" in decision.lower():
            return Command(goto="clarify_with_user")
        elif "research" in decision.lower() or "search" in decision.lower():
            return Command(goto="research_supervisor") 
        elif "report" in decision.lower() or "complete" in decision.lower():
            return Command(goto="final_report_generation")
        else:
            return Command(goto="research_supervisor")
    
    async def _execute_simple(self, user_message: str, state: AgentState, config: Configuration) -> Command:
        """–ü—Ä–æ—Å—Ç–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –±–µ–∑ streaming"""
        
        print("üß† SGR reasoning (simple mode)...")
        
        # –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏
        if len(user_message) < 10:
            return Command(goto="clarify_with_user")
        elif any(keyword in user_message.lower() for keyword in ["report", "summary", "conclude"]):
            return Command(goto="final_report_generation")
        else:
            return Command(goto="research_supervisor")
    
    async def _analyze_research_need(self, user_message: str, config: Configuration) -> str:
        """–ê–Ω–∞–ª–∏–∑ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ —á–µ—Ä–µ–∑ LLM"""
        
        try:
            from langchain.chat_models import init_chat_model
            from open_deep_research.utils import get_api_key_for_model
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
            model_config = {
                "model": config.research_model,
                "max_tokens": 200,
                "api_key": get_api_key_for_model(config.research_model, {"configurable": {}}),
                "tags": ["langsmith:nostream"]
            }
            
            model = init_chat_model(
                configurable_fields=("model", "max_tokens", "api_key"),
            ).with_config(model_config)
            
            # –ü—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            analysis_prompt = f"""
            Analyze this user request and determine the best next action:
            
            User request: "{user_message}"
            Date: {get_today_str()}
            
            Choose ONE of these actions:
            1. "clarification" - if the request is unclear or needs more details
            2. "research" - if we need to search for information
            3. "report" - if we have enough information to generate a report
            
            Respond with just the action name and brief reasoning.
            """
            
            response = await model.ainvoke([HumanMessage(content=analysis_prompt)])
            return response.content
            
        except Exception as e:
            print(f"Error in LLM analysis: {e}")
            return "research"  # Default fallback


class SGRDecisionRouter:
    """–†–æ—É—Ç–µ—Ä –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ SGR –∞–Ω–∞–ª–∏–∑–∞"""
    
    @staticmethod
    def route_sgr_decision(state: AgentState) -> str:
        """–ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ SGR —Ä–µ—à–µ–Ω–∏—è"""
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏
        messages = state.get("messages", [])
        
        if not messages:
            return "clarify"
        
        last_message = messages[-1]
        content = ""
        
        if hasattr(last_message, 'content'):
            content = last_message.content.lower()
        else:
            content = str(last_message).lower()
        
        # –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏
        if any(word in content for word in ["unclear", "clarify", "question", "what"]):
            return "clarify"
        elif any(word in content for word in ["search", "research", "find", "investigate"]):
            return "research"
        elif any(word in content for word in ["report", "summary", "complete", "finish"]):
            return "report"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç—å research_brief
        research_brief = state.get("research_brief", "")
        if not research_brief:
            return "research"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ notes
        notes = state.get("notes", [])
        if len(notes) < 2:
            return "research"
        
        return "report"


class SGRStreamingConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è SGR streaming –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
    
    def __init__(self, base_config: dict):
        self.base_config = base_config
        
    def get_streaming_config(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è SGR streaming"""
        return {
            "streaming_enabled": self.base_config.get("STREAMING_ENABLED", True),
            "display_type": self.base_config.get("STREAMING_DISPLAY_TYPE", "enhanced"),
            "update_interval": self.base_config.get("STREAMING_UPDATE_INTERVAL", 0.1),
            "animation_speed": self.base_config.get("STREAMING_ANIMATION_SPEED", 1.0),
            "schema_validation": self.base_config.get("SGR_SCHEMA_VALIDATION", True),
            "max_reasoning_steps": self.base_config.get("SGR_MAX_REASONING_STEPS", 4),
            "confidence_threshold": self.base_config.get("SGR_CONFIDENCE_THRESHOLD", 0.7),
            "enable_monitor": self.base_config.get("ENABLE_LIVE_MONITOR", True),
            "enable_tracker": self.base_config.get("ENABLE_STEP_TRACKER", True),
            "enable_progress": self.base_config.get("ENABLE_PROGRESS_BAR", True)
        }